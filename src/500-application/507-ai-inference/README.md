---
title: AI Inference Service
description: Production-ready AI inference service with dual-backend machine learning capabilities for edge computing, supporting ONNX Runtime and Candle (pure Rust) inference engines with MQTT integration
author: Edge AI Team
ms.date: 2025-10-17
ms.topic: how-to
estimated_reading_time: 8
keywords:
  - ai inference
  - onnx runtime
  - candle
  - rust
  - mqtt
  - edge computing
  - azure iot operations
  - machine learning
  - docker compose
  - kubernetes
---

A production-ready AI inference service that provides dual-backend machine learning capabilities for edge computing environments. Supports both ONNX Runtime and Candle (pure Rust) inference engines with MQTT integration for real-time processing.

## Prerequisites

### For Development

**Recommended**: Use the provided VS Code devcontainer for the best development experience. The devcontainer includes all required tools and dependencies.

1. **Open in devcontainer**:
   - VS Code: Reopen the repository in the devcontainer when prompted
   - All tools (Rust, Docker, kubectl, etc.) are pre-installed

2. **Manual setup** (if not using devcontainer):
   - Docker and Docker Compose
   - Rust 1.89+ toolchain
   - MQTT broker (Mosquitto) for local testing

### For Production Deployment

- Azure IoT Operations cluster (compatible with version in main branch)
- Azure Container Registry access
- kubectl configured for your cluster

For complete getting started guide, see the [main repository documentation](../../../docs/getting-started/).

## Architecture Overview

This component implements a scalable AI inference service designed for industrial edge computing scenarios. It features:

- **Dual Backend Support**: ONNX Runtime (220ms avg) and Candle (155ms avg) inference engines
- **MQTT Integration**: Azure IoT Operations SDK with enhanced connection resilience
- **Real-time Processing**: Optimized inference times for edge deployment, averaging 220ms for ONNX Runtime and 155ms for Candle
- **Production Ready**: 24.8MB container size with comprehensive monitoring

## Directory Structure

```text
507-ai-inference/
â”œâ”€â”€ docker-compose.yaml          # Local development environment
â”œâ”€â”€ services/                    # Service implementations
â”‚   â”œâ”€â”€ ai-edge-inference/       # Main inference service (Rust)
â”‚   â””â”€â”€ ai-edge-inference-crate/ # Shared Rust crate
â”œâ”€â”€ charts/                      # Kubernetes deployment manifests
â”‚   â”œâ”€â”€ base/                    # Base Kubernetes resources
â”‚   â””â”€â”€ model-downloader-job.yaml
â””â”€â”€ resources/                   # Configuration and model files
    â”œâ”€â”€ model_configs/           # Model configuration files
    â”œâ”€â”€ models/                  # ML model files (if present)
    â””â”€â”€ mosquitto.conf           # MQTT broker configuration
```

## Quick Start

### Local Development

1. **Ensure prerequisites** (if not using devcontainer, install manually):

   ```bash
   # Start local MQTT broker (if not already running)
   docker run -d -p 1883:1883 --name mosquitto eclipse-mosquitto:latest
   ```

2. **Start the development environment:**

   ```bash
   cd src/500-application/507-ai-inference
   
   # Default: ONNX Runtime backend
   docker-compose up --build
   
   # Or specify backend explicitly
   AI_BACKEND=onnx docker-compose up --build   # ONNX Runtime (default)
   AI_BACKEND=candle docker-compose up --build # Candle (pure Rust)
   ```

   > **Note**: This configuration uses the MQTT broker running on the host via `host.docker.internal` (port 1883).

3. **Test inference with sample image:**

   ```bash
   # Send test message to the AI inference service
   mosquitto_pub -h localhost -p 1883 -t "edge-ai/test/facility/camera/snapshots" -m '{
     "image_data": "base64_encoded_image_here",
     "device_id": "test-camera-01",
     "timestamp": "'$(date +%s)'"
   }'
   ```

4. **Monitor results:**

   ```bash
   # Subscribe to inference results
   mosquitto_sub -h localhost -p 1883 -t "edge-ai/+/+/ai/inference/+"
   ```

### Production Deployment

Deploy to Kubernetes using the provided manifests:

```bash
kubectl apply -k charts/base/
```

## Configuration

### Environment Variables

| Variable              | Description               | Default                                               |
|-----------------------|---------------------------|-------------------------------------------------------|
| `AIO_BROKER_HOSTNAME` | MQTT broker hostname      | `host.docker.internal`                                |
| `AIO_BROKER_TCP_PORT` | MQTT broker port          | `1883`                                                |
| `MQTT_INPUT_TOPICS`   | Input topic patterns      | `edge-ai/+/+/camera/snapshots`                        |
| `TOPIC_PREFIX`        | Output topic prefix       | `edge-ai/business_unit/facility/gateway_id`           |
| `DEFAULT_BACKEND`     | Default inference backend | `onnx`                                                |
| `ENABLE_DUAL_BACKEND` | Enable backend comparison | `true`                                                |
| `MODEL_CONFIG_PATH`   | Model configuration file  | `/app/resources/model_configs/industrial-safety.yaml` |
| `RUST_LOG`            | Logging level             | `info,ai_edge_inference=debug`                        |

### Topic Structure

```bash
# Input Topics
edge-ai/business_unit/facility/gateway_id/device_id/camera/snapshots
edge-ai/business_unit/facility/gateway_id/device_id/sensors/temperature

# Output Topics
edge-ai/business_unit/facility/gateway_id/device_id/ai/inference/vision
edge-ai/business_unit/facility/gateway_id/device_id/ai/inference/sensor
edge-ai/business_unit/facility/gateway_id/device_id/ai/status
```

## Model Support

### Supported Models

- **TinyYOLOv2**: Object detection with bounding boxes (63MB model)
- **MobileNet**: Image classification optimized for edge devices
- **Industrial Safety**: Custom safety detection model for industrial environments

> **Note**: Development placeholders are included in `resources/models/` to allow the service to build and run without errors. Replace with actual ONNX models for production use.

### Model Configuration

Models are configured via YAML files in `resources/model_configs/`:

```yaml
# Example: industrial-safety.yaml
name: "industrial-safety-detector"
model_path: "/models/industrial-safety.onnx"
preprocessing:
  resize: [224, 224]
  normalize: true
confidence_threshold: 0.7
```

### Adding Production Models

Replace placeholder models in `resources/models/` with actual ONNX models:

```bash
# Example: Replace with real model
cp /path/to/real/model.onnx resources/models/default.onnx

# Update configuration
vi resources/model_configs/industrial-safety.yaml

# Restart service
docker-compose restart ai-edge-inference
```

**Model Requirements**: ONNX format, compatible with ONNX Runtime 1.15+, recommended size < 50MB for repository storage.

## Performance Metrics

- **ONNX Runtime**: 220ms average inference time
- **Candle Backend**: 155ms average inference time
- **Container Size**: 24.8MB optimized for edge deployment
- **Memory Usage**: < 512MB typical operation
- **Throughput**: 4-6 inferences/second per backend

## Development

### Building Services

```bash
# Build with specific backend
docker build --build-arg BACKEND=onnx -t ai-edge-inference:onnx ./services
docker build --build-arg BACKEND=candle -t ai-edge-inference:candle ./services

# Or use docker-compose
AI_BACKEND=onnx docker-compose build
AI_BACKEND=candle docker-compose build

# Build shared crate directly
cd services/ai-edge-inference-crate
cargo build --release
```

**Backend Characteristics:**

- **ONNX**: ~200-300MB image, full ONNX model support, GPU acceleration capable
- **Candle**: ~50-100MB image, pure Rust, fastest startup, resource-constrained environments

### Testing

```bash
# Run unit tests
cd services/ai-edge-inference
cargo test

# Run integration tests
./tests/test-dual-backend-real.sh

# Run performance benchmarks
./tests/test-real-dual-backend-comprehensive.sh
```

### Adding New Models

1. Add model file to `resources/models/`
2. Create configuration in `resources/model_configs/`
3. Update service configuration
4. Test with development environment

## Monitoring and Observability

The service provides comprehensive monitoring capabilities:

- **Health Checks**: HTTP endpoint at `/health`
- **Metrics**: Prometheus-compatible metrics at `/metrics`
- **Logging**: Structured JSON logging with configurable levels
- **Tracing**: OpenTelemetry integration for distributed tracing

## Security Considerations

- MQTT connections support TLS encryption
- Model validation and sandboxing
- Input sanitization for image data
- Resource limits for inference workloads

## Troubleshooting

### Common Issues

1. **MQTT Connection Failed**: Check broker hostname and port configuration
2. **Model Loading Error**: Verify model file path and permissions
3. **High Memory Usage**: Adjust model batch size or enable model sharing
4. **Slow Inference**: Check GPU availability and model optimization

### Debug Commands

```bash
# Check service logs
docker-compose logs ai-edge-inference

# Test MQTT connectivity
docker-compose exec ai-edge-inference mosquitto_pub -h host.docker.internal -t test -m "hello"

# Validate model configuration
docker-compose exec ai-edge-inference cat /app/resources/model_configs/industrial-safety.yaml
```

## Contributing

See the main repository [CONTRIBUTING.md](/CONTRIBUTING.md) for development guidelines and contribution process.

## License

This component is part of the edge-ai project and follows the same licensing terms.

---

_AI and automation capabilities described in this scenario should be implemented following responsible AI principles, including fairness, reliability, safety, privacy, inclusiveness, transparency, and accountability. Organizations should ensure appropriate governance, monitoring, and human oversight are in place for all AI-powered solutions._

<!-- markdownlint-disable MD036 -->

_ðŸ¤– Crafted with precision by âœ¨Copilot following brilliant human instruction,
then carefully refined by our team of discerning human reviewers._

<!-- markdownlint-enable MD036 -->
