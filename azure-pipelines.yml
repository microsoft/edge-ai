# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

---
trigger:
  branches:
    include:
      - main

schedules:
  - cron: "0 5 * * *"
    displayName: Daily 9 PM PST/5 AM UTC Trigger
    branches:
      include:
        - main
    always: true

# set isMain to true if the branch is main to trigger main stage
# set isScheduled to true if the build is scheduled to trigger
# define the Megalinter cache path for the pipeline
variables:
  isMain: $[eq(variables['Build.SourceBranch'], 'refs/heads/main')]
  isScheduled: $[eq(variables['Build.Reason'], 'Schedule')]
  MegalinterCachePath: "/mnt/storage/sdc/cache/images/megalinter"

# Define the default pool for the entire pipeline. This is a
# Managed DevOps Pool with Ubuntu 2022 and WIndows 2022 agents
pool:
  name: ai-on-edge-managed-pool
  vmImage: ubuntu-latest
  demands:
    - WorkFolder -equals /mnt/storage/sdc

stages:
  # Daily scheduled build to update the MegaLinter cache
  - stage: Scheduled
    displayName: Megalinter Scheduled Caching
    condition: eq(variables.isScheduled, true)
    jobs:
      - job: UpdateMegaLinterCache
        displayName: "Update MegaLinter Cache"

        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
          demands:
            - WorkFolder -equals /mnt/storage/sdc
        steps:
          # Clear the cache directory ahead of the refresh
          - script: |
              # Clear the cache data directory ahead of the refresh
              rm -rf $(MegalinterCachePath)
              mkdir -p $(MegalinterCachePath)
            displayName: "Clear Megalinter Cache Directory"

          # Pull MegaLinter docker image
          - script: |
              # Pull MegaLinter docker image
              docker pull oxsecurity/megalinter:v8
            displayName: "Pull MegaLinter"

          # Save MegaLinter docker image to cache
          - script: |
              echo "Saving Megalinter Docker image to cache..."
              docker save oxsecurity/megalinter:v8 -o $(MegalinterCachePath)/megalinterv8-cache.tar
            displayName: "Save MegaLinter Docker Image to Cache"

  - stage: Main
    displayName: Main Build
    # dependsOn: string | [ string ]
    # Run the main build process only if the build is not scheduled and is the main branch
    condition: and(eq(variables.isMain, true), eq(variables.isScheduled, false))
    # pool: string | pool
    # variables: { string: string } | [ variable | variableReference ]
    jobs:
      - job: DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo
          - checkout: self

          # Initialize dependency scan and fail the build if it does not pass
          - task: ms.vss-governance-buildtask.governance-build-task-component-detection.ComponentGovernanceComponentDetection@0
            displayName: "Component Detection"
            inputs:
              failOnAlert: true

      # Run MegaLinter to detect linting and security issues
      - job: MegaLinter
        # Declare the job depends on the DependencyScan job to get the agent name
        # set the agent name as a demand to ensure the same agent runs all tasks
        dependsOn: DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
          demands:
            - WorkFolder -equals /mnt/storage/sdc

        steps:
          # Checkout repo
          - checkout: self

          # Attempt to restore MegaLinter from the cache otherwise pull it
          - script: |
              if [ -d "$(MegalinterCachePath)" ]; then
                if [ -f "$(MegalinterCachePath)/megalinter-cache.tar" ]; then
                  echo "Loading MegaLinter from cache..."
                  docker load -i $(MegalinterCachePath)/megalinter-cache.tar
                else
                  echo "Megalinter cache not found. Pulling MegaLinter from Docker registry..."
                  docker pull oxsecurity/megalinter:v8
                fi
              else
                echo "Cache directory $(MegalinterCachePath) does not exist. Pulling MegaLinter from Docker registry..."
                docker pull oxsecurity/megalinter:v8
              fi
            displayName: MegaLinter Cache Restore
            condition: not(canceled())

          # Run MegaLinter - for the main branch the reporter needs to be disabled
          - script: |
              docker run -v $(System.DefaultWorkingDirectory):/tmp/lint \
                --env-file <(env | grep -e SYSTEM_ -e BUILD_ -e TF_ -e AGENT_) \
                -e SYSTEM_ACCESSTOKEN=$(System.AccessToken) \
                -e GIT_AUTHORIZATION_BEARER=$(System.AccessToken) \
                -e AZURE_COMMENT_REPORTER='false' \
                oxsecurity/megalinter:v8
            displayName: Run MegaLinter

  - stage: PR
    displayName: PR Build Process
    # dependsOn: string | [ string ]
    # Run the PR build process only if the build is not scheduled and not the main branch
    condition: and(eq(variables.isMain, false), eq(variables.isScheduled, false))
    # pool: string | pool
    # variables: { string: string } | [ variable | variableReference ]
    jobs:
      # Ensure a clean workspace for the PR build process
      - job: PrePRCleanup
        displayName: Pre-PR Workspace Cleanup
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          - task: DeleteFiles@1
            displayName: "Delete files from $(Agent.WorkFolder)"
            inputs:
              SourceFolder: "$(Agent.WorkFolder)"

      # Run Dependency Scan based on the MSFT vss governance build task.
      # A version of this task is also auto injected by corporate policy.
      # It's declared here to make it explicit in the pipeline.
      - job: DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        dependsOn:
          - PrePRCleanup
        steps:
          # Checkout repo
          - checkout: self

          # Initialize dependency scan and fail the build if it does not pass
          - task: ms.vss-governance-buildtask.governance-build-task-component-detection.ComponentGovernanceComponentDetection@0
            displayName: "Component Detection"
            inputs:
              failOnAlert: true

      # Run MegaLinter to detect linting and security issues
      - job: MegaLinter
        # This should also only run if the DependencyScan job passes
        dependsOn:
          - DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
          demands:
            - WorkFolder -equals /mnt/storage/sdc
        steps:
          # Checkout repo
          - checkout: self

          # Attempt to restore MegaLinter from the cache otherwise pull it
          - script: |
              if [ -d "$(MegalinterCachePath)" ]; then
                if [ -f "$(MegalinterCachePath)/megalinter-cache.tar" ]; then
                  echo "Loading MegaLinter from cache..."
                  docker load -i $(MegalinterCachePath)/megalinter-cache.tar
                else
                  echo "Megalinter cache not found. Pulling MegaLinter from Docker registry..."
                  docker pull oxsecurity/megalinter:v8
                fi
              else
                echo "Cache directory $(MegalinterCachePath) does not exist. Pulling MegaLinter from Docker registry..."
                docker pull oxsecurity/megalinter:v8
              fi
            displayName: MegaLinter Cache Restore
            condition: not(canceled())

          # Run MegaLinter
          # See here for required system props: https://github.com/oxsecurity/megalinter/blob/0f18f72065ffdeec09b616087b8298a3b13b2dee/megalinter/reporters/AzureCommentReporter.py#L7C1-L13C22
          - script: |
              docker run -v $(System.DefaultWorkingDirectory):/tmp/lint \
                --env-file <(env | grep -e SYSTEM_ -e BUILD_ -e TF_ -e AGENT_) \
                -e SYSTEM_ACCESSTOKEN=$(System.AccessToken) \
                -e GIT_AUTHORIZATION_BEARER=$(System.AccessToken) \
                -e SYSTEM_COLLECTIONURI='$(System.CollectionUri)' \
                -e SYSTEM_PULLREQUEST_PULLREQUESTID='$(System.PullRequest.PullRequestId)' \
                -e SYSTEM_PULLREQUEST_SOURCEREPOSITORYURI='$(System.PullRequest.SourceRepositoryURI)' \
                -e SYSTEM_TEAMPROJECT='$(System.TeamProject)' \
                -e BUILD_BUILDID='$(Build.BuildId)' \
                -e BUILD_REPOSITORY_ID='$(Build.Repository.ID)' \
                -e AZURE_COMMENT_REPORTER='true' \
                oxsecurity/megalinter:v8
            displayName: Run MegaLinter

          # Upload MegaLinter reports as artifacts
          - task: PublishPipelineArtifact@1
            condition: succeededOrFailed()
            displayName: Upload MegaLinter reports
            inputs:
              targetPath: "$(System.DefaultWorkingDirectory)/megalinter-reports/"
              artifactName: MegaLinterReport

      # Run Terraform documentation check to prompt committer to update
      # auto-generated documentation if changes are detected
      - job: TerraformDocsCheck
        dependsOn:
          - MegaLinter
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo
          - checkout: self

          - bash: |
              # Install terraform-docs
              curl -sSL https://github.com/terraform-docs/terraform-docs/releases/download/v0.16.0/terraform-docs-v0.16.0-linux-amd64.tar.gz | tar -xz -C /usr/local/bin

              # Ensure Terraform Docs update script is set to executable (just in case)
              chmod +x $(System.DefaultWorkingDirectory)/src/update-all-terraform-docs.sh

              # Run documentation update Terrform
              $(System.DefaultWorkingDirectory)/src/update-all-terraform-docs.sh

              # Check for changes in README.md files
              changed_files=$(git diff --name-only)
              readme_changed=false
              for file in $changed_files; do
                if [[ $file == src/*/README.md ]]; then
                  if head -n 1 "$file" | grep -q "^<!-- BEGIN_TF_DOCS -->$"; then
                    echo "Updates required for: $(System.DefaultWorkingDirectory)/$file"
                    readme_changed=true
                  fi
                fi
              done

              # Check if there are any changes in the Terraform documentation
              if [ "$readme_changed" = true ]; then
                echo "Updates are required for Terraform documentation."
                echo "Please go into the project's src directory, run the update-all-terraform-docs.sh script, and commit changes."
                echo "##vso[task.logissue type=error]Terraform auto-gen documentation needs to be updated. Please run the update-all-terraform-docs.sh script and commit the changes."
                exit 1
              else
                echo "No updates detected in the Terraform documentation."
              fi
            displayName: Check for changes in terraform docs
            workingDirectory: $(System.DefaultWorkingDirectory)
            condition: and( not(failed()), not(canceled()) )

      # This job checks for changes in the src directory and runs specific
      # tasks based on the changes it discovers. Primarily for bash, pwsh and terraform.
      - job: MatrixBuildFolderCheck
        displayName: Check for changes in src directory
        dependsOn:
          - TerraformDocsCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo and set fetchDepth to 0 to get all history
          # In large codebases, this can be slow, but it's necessary to get all the history
          - checkout: self
            fetchDepth: "0"

          # Check for changes in the src directory
          - bash: |
              echo "Checking for changes in the ./src/000-subscription folder..."
              # Get the list of changed files between the current branch and main
              changed_files=$(git diff --name-only --diff-filter=ACMRT origin/main...HEAD)

              # check if shell files in subscription setup folder have changed
              if echo "$changed_files" | grep -q 'src/000-subscription/.*\.sh$'; then
                echo "Changes detected in the ./src/000-subscription folder shell files."
                echo "##vso[task.setvariable variable=changesInRpEnablementShell;isOutput=true]true"
              else
                echo "No changes detected in the ./src/000-subscription folder shell files."
                echo "##vso[task.setvariable variable=changesInRpEnablementShell;isOutput=true]false"
              fi

              # check if Pwsh files in subscription setup folder have changed
              if echo "$changed_files" | grep -q 'src/000-subscription/.*\.ps1$'; then
                echo "Changes detected in the ./src/000-subscription folder pwsh files."
                echo "##vso[task.setvariable variable=changesInRpEnablementPwsh;isOutput=true]true"
              else
                echo "No changes detected in the ./src/000-subscription folder pwsh files."
                echo "##vso[task.setvariable variable=changesInRpEnablementPwsh;isOutput=true]false"
              fi

              # Check if Terraform files, in any src directory that has tf files, have changed
              # This command will create a JSON object with the first-level folder names as keys
              # which will be used to create a matrix of jobs to run Terraform tasks for each folder
              # see the Azure DevOps documentation for more information on matrix jobs using JSON objects
              # https://learn.microsoft.com/azure/devops/pipelines/yaml-schema/jobs-job-strategy?view=azure-pipelines#remarks
              # This will output a JSON object as follows:
              # {
              #   "005-onboard-reqs": {
              #     "folderName": "005-onboard-reqs"
              #   },
              #   "010-vm-host": {
              #     "folderName": "010-vm-host"
              #   },
              #   "020-cncf-cluster": {
              #     "folderName": "020-cncf-cluster"
              #   },
              #   "030-iot-ops-cloud-reqs": {
              #     "folderName": "030-iot-ops-cloud-reqs"
              #   },
              #   "040-iot-ops": {
              #     "folderName": "040-iot-ops"
              #   },
              #   "050-messaging": {
              #     "folderName": "050-messaging"
              #   }
              # }
              echo "Checking for changes in the first-level folders under src with .tf, .tfvars, .tfstate, or .hcl file changes..."
              changed_tf_folders=$(echo "$changed_files" |  # Get the list of changed files
                grep -E 'src/[^/]+/.*\.(tf|tfvars|tfstate|hcl)$' |  # Filter for .tf, .tfvars, .tfstate, or .hcl files in src directory
                awk -F'/' '{print $2}' |  # Extract the first-level folder names
                sort -u |  # Remove all duplicates
                sort -n |  # Sort the folder names numerically so they can be applied in order
                jq -R -s -c 'split("\n") |  # Convert the newline-separated list into a JSON array
                map(select(length > 0)) |  # Remove all empty entries
                map({key: ., value: {folderName: .}}) |  # Create JSON objects with folder name as keys, the keys are used to populate the AzDO job name
                from_entries')  # Convert entries to a JSON object

              if [ "$(echo $changed_tf_folders | jq 'length')" -ne 0 ]; then
                echo "Changes detected in the following Terraform folders: $changed_tf_folders"
                echo "##vso[task.setvariable variable=changesInInstall;isOutput=true]true"
                echo "##vso[task.setvariable variable=changedTFFolders;isOutput=true]$changed_tf_folders"
              else
                echo "No changes detected in the first-level folders under src with .tf, .tfvars, .tfstate, or .hcl file changes."
                echo "##vso[task.setvariable variable=changesInInstall;isOutput=true]false"
                echo "##vso[task.setvariable variable=changedTFFolders;isOutput=true]none"
              fi

            displayName: Check for changes in src folders
            name: matrixBuildFolderCheckTask

      # Job to run shell script tests for the resource providers if they have changed
      - job: ResourceProviderShellScriptTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        # Check if the changeset includes 000-subscription files and run if it does.
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInRpEnablementShell'], 'true')
        steps:
          - task: AzureCLI@2
            displayName: Azure CLI for Resource Provider Unregistration Script Test
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: "$(System.DefaultWorkingDirectory)/src/000-subscription"
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                echo "uninstalling azure providers"
                ./unregister-azure-providers.sh aio-azure-resource-providers.txt
                echo "uninstalled azure providers"
            condition: and( not(failed()), not(canceled()) )

          - task: AzureCLI@2
            displayName: Azure CLI for Resource Provider Registration Script Test
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: "$(System.DefaultWorkingDirectory)/src/000-subscription"
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                echo "installing azure providers"
                ./register-azure-providers.sh aio-azure-resource-providers.txt
                echo "installed azure providers"
            condition: and( not(failed()), not(canceled()) )

      # Job to run Pwsh script tests for the resource providers if they have changed
      - job: ResourceProviderPWSHScriptTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: "windows-2022"
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInRpEnablementPwsh'], 'true')
        variables:
          testResultsOutput: "$(System.DefaultWorkingDirectory)/PWSH-TEST-RESULTS.xml"
        steps:
          # Run Pester tests for the resource provider scripts
          - powershell: ./.azdo/subscripts/Invoke-Pester.ps1 -Path ./src/000-subscription -OutputFile $(testResultsOutput)
            displayName: "Run pester"
            workingDirectory: $(System.DefaultWorkingDirectory)

          # Publish the Pester test results
          - task: PublishTestResults@2
            displayName: Publish Test Results
            inputs:
              testRunTitle: "Test Results for Pester"
              buildPlatform: "Windows"
              testRunner: "NUnit"
              testResultsFiles: "$(testResultsOutput)"
              failTaskOnFailedTests: true

      # Job to run Terraform install, init, provider version checks, plan,
      # and tests for the changed TF folders
      - job: TerraformClusterTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInInstall'], 'true')
        strategy:
          matrix:
            # Dynamically create a job for each folder in the changedTFFolders variable
            # there is some magic here as the changedTFFolders variable is a JSON object
            # Azure Pipeline strategy matrix can automatically parse JSON objects into
            # an enumerated matrix of values. This will create a $folderName variable
            # that can be used in the job steps to reference the current TF folder name
            # being processed.
            $[ dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changedTFFolders'] ]
          maxParallel: 1

        steps:
          # Initialize Terraform
          - task: TerraformInstaller@1
            displayName: "Install Terraform"
            inputs:
              terraformVersion: latest

          # Run terraform init with the established service connection in AzDO Pipeline config
          - task: TerraformCLI@1
            displayName: "Terraform Init"
            inputs:
              command: init
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              runAzLogin: true
              environmentServiceName: "azdo-ai-for-edge-iac-for-edge"
              backendType: azurerm
              # Service connection to authorize backend access. Supports Subscription & Management Group Scope
              backendServiceArm: "azdo-ai-for-edge-iac-for-edge"
              backendAzureRmResourceGroupName: "IaC_For_Edge"
              # azure location shortname of the backend resource group and storage account
              backendAzureRmResourceGroupLocation: "eastus"
              backendAzureRmStorageAccountName: "iacforedgetf"
              # azure blob container to store the state file
              backendAzureRmContainerName: "iacforedgetf"
              # azure blob file name
              backendAzureRmKey: infrax.tfstate

          # Check TF Provider versions for this folder and output build warnings if issues are detected
          - task: AzureCLI@2
            displayName: "Terraform Provider version check"
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                # Call TF version command and parse the output
                version_data=$(terraform version -json)
                echo "$version_data" | jq .
                # set outdated if there is a global TF update available
                outdated=$(echo "$version_data" | jq -r '.terraform_outdated')
                # Parse the provider selections and build an array to check for updates
                # This section will parse the provider subobject and extract the provider name
                # and version.
                # example object:
                # {
                #  "terraform_version": "1.10.5",
                #  "platform": "linux_amd64",
                #  "provider_selections": {
                #    "registry.terraform.io/hashicorp/azuread": "3.1.0",
                #    "registry.terraform.io/hashicorp/azurerm": "4.16.0",
                #    "registry.terraform.io/hashicorp/random": "3.6.3"
                #  },
                #  "terraform_outdated": false
                # }
                provider_details=$(echo "$version_data" | jq -r '
                  .provider_selections |
                  to_entries[] |
                  "\(.key) \
                  \(.key | split("/") | .[1]) \
                  \(.key | split("/") | .[2]) \
                  \(.value)"
                ')

                # Loop through the provider details and check for updates
                # by calling the tf registry API and comparing the versions
                while IFS= read -r line; do

                  # Check if the pipeline is canceled, and exit if so
                  if [ "$AGENT_JOBSTATUS" = "Canceled" ]; then
                    echo "Pipeline is canceled. Exiting..."
                    exit 0
                  fi

                  registry=$(echo "$line" | awk '{print $1}')
                  source=$(echo "$line" | awk '{print $2}')
                  provider=$(echo "$line" | awk '{print $3}')
                  version=$(echo "$line" | awk '{print $4}')
                  url="https://registry.terraform.io/v1/providers/$source/$provider/versions"
                  response=$(curl -s "$url")

                  # Check versions
                  latest_version=$(echo "$response" | jq -r '.versions[].version' | sort -V | tail -n 1)
                  if [ "$(printf '%s\n' "$version" "$latest_version" | sort -V | tail -n 1)" != "$version" ]; then
                    # Log a build warning if the provider version is outdated
                    echo "##vso[task.logissue type=warning]A newer version ($latest_version) is available for provider $provider from $source. Specified version: $version."
                  else
                    echo "Terraform provider: $provider version is up to date."
                  fi
                done <<< "$provider_details"

                # If the global TF version is outdated, log a build warning
                if [ "$outdated" = "true" ]; then
                  echo "##vso[task.logissue type=warning]Terraform version is outdated."
                else
                  echo "Terraform version is up to date."
                fi
            condition: and( not(failed()), not(canceled()) )

          # Run Terraform Validate to ensure the TF files are valid
          # break if validation fails
          - task: TerraformCLI@1
            displayName: "Terraform Validate"
            inputs:
              command: validate
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
            continueOnError: false

          # Parse variables.tf and construct the specific commandOptions required for the plan call
          - bash: |
              # get the tf variable file from the current matrix directory
              variables_file=$(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/variables.tf
              # start the base command option for the coming TF Plan
              command_options="-out=$(Build.ArtifactStagingDirectory)/$(Build.Buildnumber).$(folderName).tfplan -detailed-exitcode"
              # if the variables file exists, parse it and for each variable,
              # see if we've set an AzDO pipeline variable to a matched name,
              # and then add that combination to that to the command options.
              # Special case is for Custom Locations OID because that is a
              # secrete value, and AzDO will not let you dynamically access
              # the variable name, you must know it, to map it.
              # e.g. https://stackoverflow.com/questions/75536144/azure-pipelines-error-tf401444-please-sign-in-at-least-once-as
              if [ -f "$variables_file" ]; then
                required_vars=$(grep -E 'variable "[^"]+" \{' $variables_file | awk -F'"' '{print $2}')
                for env_var in $(printenv | awk -F= '{print $1}'; echo "TF_VAR_CUSTOM_LOCATIONS_OID"); do
                  var_name=${env_var#TF_VAR_}
                  if echo "$required_vars" | grep -q "^${var_name,,}$"; then
                    command_options="$command_options -var ${var_name,,}=${!env_var}"
                  fi
                done
                # secretes will be automatically masked by AzDO
                echo $command_options
              fi
              echo "##vso[task.setvariable variable=commandOptions;isOutput=true]$command_options"
            displayName: "Parse variables.tf and build commandOptions"
            name: parseVariables
            condition: and( not(failed()), not(canceled()) )

          # Run Terraform Plan for reporting
          - task: TerraformCLI@1
            displayName: "Terraform Plan"
            inputs:
              command: plan
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              runAzLogin: true
              environmentServiceName: "azdo-ai-for-edge-iac-for-edge"
              # this publish will drive the Terraform Plan UI on each build
              publishPlanResults: "$(folderName) - Terraform Test Plan"
              commandOptions: $(parseVariables.commandOptions)
            condition: and( not(failed()), not(canceled()) )

          # Publish the Terraform Plan as an official build artifact for downloading
          - task: PublishBuildArtifacts@1
            displayName: "Publish Artifact: $(folderName) Terraform Plan"
            inputs:
              PathtoPublish: "$(Build.ArtifactStagingDirectory)/$(Build.Buildnumber).$(folderName).tfplan"
              ArtifactName: "TerraformPlan"

          # Terraform Test Command
          # This is not yet implemented, tracking here: https://github.com/jason-johnson/azure-pipelines-tasks-terraform/issues/398
          # Once this command is available, the AZ CLI below can be removed
          # - task: TerraformCLI@2
          #   displayName: 'terraform test'
          #   inputs:
          #     command: test
          #     workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/tests"
          #     commandOptions: "-filter=cluster.tftest.hcl"

          # Run Terraform Test command using the Azure CLI
          # NOTE: For this task to work it needs the subscription id set for the AzureAD provider
          # beyond the integrated MSI.
          - task: AzureCLI@2
            name: TerraformTest
            displayName: Azure CLI for Terraform CLI test run
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |
                # Export the ARM subscription ID so Terraform tests can be executed
                export ARM_SUBSCRIPTION_ID=$(SUBSCRIPTION_ID)

                # Define the log file from the test output to read up
                log_file="$(System.DefaultWorkingDirectory)/$(folderName)_test_output.log"
                echo "Writing Terraform test output to $log_file"

                # run TF tests and push results to log file
                terraform test -json -var custom_locations_oid=$(TF_VAR_CUSTOM_LOCATIONS_OID) > $log_file
                unset ARM_SUBSCRIPTION_ID

                # Read each line of the log file and extract the @timestamp and @message fields
                # this will be output to console in the build system
                while IFS= read -r line; do
                  echo "$line" | jq -r '"\(.["@timestamp"]) .... \(.["@message"])"'
                done < "$log_file"

                # Get the last line of the output and parse it for the word "Success"
                last_line=$(tail -n 1 "$log_file")
                if echo "$last_line" | jq -e '.["@message"]' | grep -q "Success"; then
                  echo "Terraform tests passed."
                else
                  echo "Terraform tests failed."
                  exit 1
                fi
            condition: and( not(failed()), not(canceled()) )

          # Need to use an external tool to convert the Terraform test output to JUnit XML
          # This tool is available as an NPM package and can be installed and run in the pipeline
          # But it requires Node.js to be installed on the agent
          - task: UseNode@1
            inputs:
              version: "22.x"
            condition: always()

          # Install the tftest-to-junitxml NPM package
          # https://github.com/Liam-Johnston/tftest-to-junitxml
          - script: |
              npm install -g tftest-to-junitxml
            displayName: "Install tftest-to-junitxml"
            condition: always()

          # Call the tool to convert the Terraform test output to JUnit XML
          - script: |
              npx tftest-to-junitxml "$(System.DefaultWorkingDirectory)/$(folderName)_test_output.log"
            displayName: "Convert Terraform test output to JUnit XML"
            condition: always()

          # Publish the Terraform test results as a build artifact to drive
          # the Test Results UI on each build. These will be merged with the
          # Pester Tests and other test results in the pipeline, but are
          # discretely visible in the Test Results UI.
          - task: PublishTestResults@2
            displayName: Publish Test Results
            inputs:
              testResultsFiles: "TEST-terraform.xml"
              testRunTitle: "Test Results for $(folderName)"
              buildPlatform: "Linux"
              testRunner: "JUnit"
              failTaskOnFailedTests: true
            condition: always()
