# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

---
trigger:
  branches:
    include:
      - main

schedules:
  - cron: "0 5 * * *"
    displayName: Daily 9 PM PST/5 AM UTC Trigger
    branches:
      include:
        - main
    always: true

# set isMain to true if the branch is main to trigger main stage
# set isScheduled to true if the build is scheduled to trigger
# define the Megalinter cache path for the pipeline
variables:
  isMain: $[eq(variables['Build.SourceBranch'], 'refs/heads/main')]
  isScheduled: $[eq(variables['Build.Reason'], 'Schedule')]
  MegalinterCachePath: "/mnt/storage/sdc/cache/images/megalinter"

# Define the default pool for the entire pipeline. This is a
# Managed DevOps Pool with Ubuntu 2022 and WIndows 2022 agents
pool:
  name: ai-on-edge-managed-pool
  vmImage: ubuntu-latest

stages:
  # Daily scheduled build to update the MegaLinter cache
  - stage: Scheduled
    displayName: Megalinter Scheduled Caching
    condition: eq(variables.isScheduled, true)
    jobs:
      - job: UpdateMegaLinterCache
        displayName: "Update MegaLinter Cache"
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
          # Make a demand for the data drive on this agent to ensure
          # we can pull megalinter from the cache
          demands:
            - WorkFolder -equals /mnt/storage/sdc/

        steps:
          # Clear the cache directory ahead of the refresh
          - script: |

              # Clear the cache data directory ahead of the refresh
              echo "Clearing Megalinter Cache Directory... $(MegalinterCachePath)"
              rm -rf $(MegalinterCachePath)
              echo "Creating Megalinter Cache Directory... $(MegalinterCachePath)"
              mkdir -p $(MegalinterCachePath)

            displayName: "Clear Megalinter Cache Directory"

          # Pull MegaLinter docker image
          - script: |
              echo "Pulling MegaLinter Docker image..."
              docker pull oxsecurity/megalinter:v8
            displayName: "Pull MegaLinter"

          # Save MegaLinter docker image to cache
          - script: |
              echo "Saving Megalinter Docker image to cache..."
              docker save oxsecurity/megalinter:v8 -o $(MegalinterCachePath)/megalinterv8-cache.tar
            displayName: "Save MegaLinter Docker Image to Cache"

  - stage: Main
    displayName: Main Build
    # dependsOn: string | [ string ]
    # Run the main build process only if the build is not scheduled and is the main branch
    condition: and(eq(variables.isMain, true), eq(variables.isScheduled, false))
    # pool: string | pool
    # variables: { string: string } | [ variable | variableReference ]
    jobs:
      - job: DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo
          - checkout: self
            clean: true

          # Initialize dependency scan and fail the build if it does not pass
          - task: ms.vss-governance-buildtask.governance-build-task-component-detection.ComponentGovernanceComponentDetection@0
            displayName: "Component Detection"
            inputs:
              failOnAlert: true

      # Use the megaLinter-template.yml to run MeagaLinter
      # This job will only run if the DependencyScan job passes.
      - template: .azdo/megalinter-template.yml
        parameters:
          dependsOn: DependencyScan
          displayName: "Run MegaLinter"
          condition: succeeded()
          megalinterCachePath: $(MegalinterCachePath)
          # for main do not report results as there's no PR to report to
          enableAzureReporter: false

      # Use the wiki-update-template.yml to update the wiki documentation
      # This job will only run if the MegaLinter job passes.
      - template: .azdo/wiki-update-template.yml
        parameters:
          dependsOn: MegaLinter
          displayName: "Wiki Documentation Update"
          condition: succeeded()
          branchRepoFolder: "branch"
          wikiRepoFolder: "wiki"
          wikiRepo: "git://edge-ai/edge-ai@refs/heads/wiki"
          wikiBranch: "wiki"

  - stage: PR
    displayName: PR Build Process
    # dependsOn: string | [ string ]
    # Run the PR build process only if the build is not scheduled and not the main branch
    condition: and(eq(variables.isMain, false), eq(variables.isScheduled, false))
    # pool: string | pool
    # variables: { string: string } | [ variable | variableReference ]
    jobs:
      # Ensure a clean workspace for the PR build process
      - job: PrePRCleanup
        displayName: Pre-PR Workspace Cleanup
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          - task: DeleteFiles@1
            displayName: "Delete files from $(Agent.WorkFolder)"
            inputs:
              SourceFolder: "$(Agent.WorkFolder)"

      # Run Dependency Scan based on the MSFT vss governance build task.
      # A version of this task is also auto injected by corporate policy.
      # It's declared here to make it explicit in the pipeline.
      - job: DependencyScan
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        dependsOn:
          - PrePRCleanup
        steps:
          # Checkout repo
          - checkout: self
            clean: true

          # Initialize dependency scan and fail the build if it does not pass
          - task: ms.vss-governance-buildtask.governance-build-task-component-detection.ComponentGovernanceComponentDetection@0
            displayName: "Component Detection"
            inputs:
              failOnAlert: true

      # Use the MegaLinter template for PR builds with Azure reporter enabled
      - template: .azdo/megalinter-template.yml
        parameters:
          dependsOn: DependencyScan
          displayName: "Run MegaLinter"
          condition: succeeded()
          megalinterCachePath: $(MegalinterCachePath)
          # for PR builds, enable the Azure reporter to report results
          # to the PR and set the pull request ID and source repo URI
          enableAzureReporter: true
          pullRequestId: $(System.PullRequest.PullRequestId)
          sourceRepoUri: $(System.PullRequest.SourceRepositoryURI)

      # Run Terraform documentation check to prompt committer to update
      # auto-generated documentation if changes are detected
      - job: TerraformDocsCheck
        dependsOn:
          - MegaLinter
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo
          - checkout: self
            clean: true

            # Install terraform-docs from GitHub releases
          - bash: |

              # Check if terraform-docs is already installed
              if ! command -v terraform-docs &> /dev/null; then
                echo "terraform-docs not found. Installing..."
                # Detect architecture
                ARCH=$(uname -m)
                case $ARCH in
                  x86_64|amd64)
                    TERRAFORM_DOCS_ARCH="amd64"
                    ;;
                  aarch64|arm64)
                    TERRAFORM_DOCS_ARCH="arm64"
                    ;;
                  *)
                    echo "Unsupported architecture: $ARCH"
                    exit 1
                    ;;
                esac

                # Install terraform-docs
                echo "Installing terraform-docs for $TERRAFORM_DOCS_ARCH architecture..."
                curl -Lo ./terraform-docs.tar.gz "https://github.com/terraform-docs/terraform-docs/releases/download/v0.16.0/terraform-docs-v0.16.0-$(uname)-$TERRAFORM_DOCS_ARCH.tar.gz"
                tar -xzf terraform-docs.tar.gz
                chmod +x terraform-docs
                sudo mv terraform-docs /usr/local/bin/
              else
                echo "terraform-docs is already installed"
                terraform-docs --version
              fi

            displayName: Install terraform-docs
            condition: and( not(failed()), not(canceled()) )

          - bash: |

              # Call tf-docs-check.sh script to check for changes
              readme_changed=$(tail -n 1 $(System.DefaultWorkingDirectory)/scripts/tf-docs-check.sh)

              # Check if there are any changes in the Terraform documentation
              if [ "$readme_changed" = true ]; then
                echo "Updates are required for Terraform documentation."
                echo "Please go into the project's src directory, run the update-all-terraform-docs.sh script, and commit changes."
                echo "##vso[task.logissue type=error]Terraform auto-gen documentation needs to be updated. Please run the update-all-terraform-docs.sh script and commit the changes."
                exit 1
              else
                echo "No updates detected in the Terraform documentation."
              fi

            displayName: Check for changes in terraform docs
            workingDirectory: $(System.DefaultWorkingDirectory)
            condition: and( not(failed()), not(canceled()) )

      # This job checks for changes in the src directory and runs specific
      # tasks based on the changes it discovers. Primarily for bash, pwsh and terraform.
      - job: MatrixBuildFolderCheck
        displayName: Check for changes in src directory
        dependsOn:
          - TerraformDocsCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          # Checkout repo and set fetchDepth to 0 to get all history
          # In large codebases, this can be slow, but it's necessary to get all the history
          - checkout: self
            fetchDepth: "0"
            clean: true

          # Check for changes in the src directory
          - bash: |

              echo "Checking for changes in the ./src/000-subscription folder..."
              # Get the list of changed files between the current branch and main
              changed_files=$(git diff --name-only --diff-filter=ACMRT origin/main...HEAD)

              # check if shell files in subscription setup folder have changed
              if echo "$changed_files" | grep -q 'src/000-subscription/.*\.sh$'; then
                echo "Changes detected in the ./src/000-subscription folder shell files."
                echo "##vso[task.setvariable variable=changesInRpEnablementShell;isOutput=true]true"
              else
                echo "No changes detected in the ./src/000-subscription folder shell files."
                echo "##vso[task.setvariable variable=changesInRpEnablementShell;isOutput=true]false"
              fi

              # check if Pwsh files in subscription setup folder have changed
              if echo "$changed_files" | grep -q 'src/000-subscription/.*\.ps1$'; then
                echo "Changes detected in the ./src/000-subscription folder pwsh files."
                echo "##vso[task.setvariable variable=changesInRpEnablementPwsh;isOutput=true]true"
              else
                echo "No changes detected in the ./src/000-subscription folder pwsh files."
                echo "##vso[task.setvariable variable=changesInRpEnablementPwsh;isOutput=true]false"
              fi

              # Check if Terraform files, in any src directory that has tf files, have changed
              # This command will create a JSON object with the first-level folder names as keys
              # which will be used to create a matrix of jobs to run Terraform tasks for each folder
              # see the Azure DevOps documentation for more information on matrix jobs using JSON objects
              # https://learn.microsoft.com/azure/devops/pipelines/yaml-schema/jobs-job-strategy?view=azure-pipelines#remarks
              # This will output a JSON object as follows:
              # {
              #   "005-onboard-reqs": {
              #     "folderName": "005-onboard-reqs"
              #   },
              #   "010-vm-host": {
              #     "folderName": "010-vm-host"
              #   },
              #   "020-cncf-cluster": {
              #     "folderName": "020-cncf-cluster"
              #   },
              #   "030-iot-ops-cloud-reqs": {
              #     "folderName": "030-iot-ops-cloud-reqs"
              #   },
              #   "040-iot-ops": {
              #     "folderName": "040-iot-ops"
              #   },
              #   "050-messaging": {
              #     "folderName": "050-messaging"
              #   }
              # }
              echo "Checking for changes in the first-level folders under src with .tf, .tfvars, .tfstate, or .hcl file changes..."
              changed_tf_folders=$(echo "$changed_files" |  # Get the list of changed files
                grep -E 'src/[^/]+/.*\.(tf|tfvars|tfstate|hcl)$' |  # Filter for .tf, .tfvars, .tfstate, or .hcl files in src directory
                awk -F'/' '{print $2}' |  # Extract the first-level folder names
                sort -u |  # Remove all duplicates
                sort -n |  # Sort the folder names numerically so they can be applied in order
                jq -R -s -c 'split("\n") |  # Convert the newline-separated list into a JSON array
                map(select(length > 0)) |  # Remove all empty entries
                map({key: ., value: {folderName: .}}) |  # Create JSON objects with folder name as keys, the keys are used to populate the AzDO job name
                from_entries')  # Convert entries to a JSON object

              if [ "$(echo $changed_tf_folders | jq 'length')" -ne 0 ]; then
                echo "Changes detected in the following Terraform folders: $changed_tf_folders"
                echo "##vso[task.setvariable variable=changesInInstall;isOutput=true]true"
                echo "##vso[task.setvariable variable=changedTFFolders;isOutput=true]$changed_tf_folders"
              else
                echo "No changes detected in the first-level folders under src with .tf, .tfvars, .tfstate, or .hcl file changes."
                echo "##vso[task.setvariable variable=changesInInstall;isOutput=true]false"
                echo "##vso[task.setvariable variable=changedTFFolders;isOutput=true]none"
              fi

            displayName: Check for changes in src folders
            name: matrixBuildFolderCheckTask

      # Job to run shell script tests for the resource providers if they have changed
      - job: ResourceProviderShellScriptTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        # Check if the changeset includes 000-subscription files and run if it does.
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInRpEnablementShell'], 'true')
        steps:
          - checkout: self
            clean: true

          - task: AzureCLI@2
            displayName: Azure CLI for Resource Provider Unregistration Script Test
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: "$(System.DefaultWorkingDirectory)/src/000-subscription"
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |

                # Uninstall the Azure providers
                echo "uninstalling azure providers"
                ./unregister-azure-providers.sh aio-azure-resource-providers.txt
                echo "uninstalled azure providers"
            condition: and( not(failed()), not(canceled()) )

          - task: AzureCLI@2
            displayName: Azure CLI for Resource Provider Registration Script Test
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: "$(System.DefaultWorkingDirectory)/src/000-subscription"
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |

                # Install the Azure providers
                echo "installing azure providers"
                ./register-azure-providers.sh aio-azure-resource-providers.txt
                echo "installed azure providers"
            condition: and( not(failed()), not(canceled()) )

      # Job to run Pwsh script tests for the resource providers if they have changed
      - job: ResourceProviderPWSHScriptTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: "windows-2022"
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInRpEnablementPwsh'], 'true')
        variables:
          testResultsOutput: "$(System.DefaultWorkingDirectory)/PWSH-TEST-RESULTS.xml"
        steps:
          - checkout: self
            clean: true

          # Run Pester tests for the resource provider scripts
          - powershell: ./scripts/Invoke-Pester.ps1 -Path ./src/000-subscription -OutputFile $(testResultsOutput)
            displayName: "Run pester"
            workingDirectory: $(System.DefaultWorkingDirectory)

          # Publish the Pester test results
          - task: PublishTestResults@2
            displayName: Publish Test Results
            inputs:
              testRunTitle: "Test Results for Pester"
              buildPlatform: "Windows"
              testRunner: "NUnit"
              testResultsFiles: "$(testResultsOutput)"
              failTaskOnFailedTests: true

      # Job to check changes in the Terraform AIO component versions and ensure that
      # they are not outdated. See ./scripts/aio-version-checker.sh for details
      - job: TfAIOVersionChecker
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          - checkout: self
            clean: true

          # Call aio-version-checker.sh script to check for changes in the TF specified AIO components
          # versions. Creates build warnings if differences between local defined and remote available are detected
          - bash: |

              # Call aio-version-checker.sh script to check for changes in the specified AIO component
              # versions. Creates build warnings if differences between local and remote are detected
              version_check_results=$(./scripts/aio-version-checker.sh)

              echo "Version Check Results: $version_check_results"

              # Split the string into an array
              # Parse the JSON and loop through each object
              echo "$version_check_results" | jq -c '.[]' | while read -r result; do
                name=$(echo "$result" | jq -r '.name')
                local_version=$(echo "$result" | jq -r '.local_version')
                remote_version=$(echo "$result" | jq -r '.remote_version')
                local_train=$(echo "$result" | jq -r '.local_train')
                remote_train=$(echo "$result" | jq -r '.remote_train')

                # Check for version mismatch
                if [[ "$local_version" != "$remote_version" ]]; then
                    echo "##vso[task.logissue type=error]Component $name has version mismatch. Local version: $local_version, Available version: $remote_version."
                fi

                # Additional train quality check
                if [[ "$local_train" != "$remote_train" ]]; then
                    echo "##vso[task.logissue type=error]Component $name has train quality mismatch. Local train: $local_train, Available train: $remote_train."
                fi
              done

            displayName: "TF AIO Version Check"
            condition: and( not(failed()), not(canceled()) )

      # Job to check uniformity differences in TF var definitions and ensure that
      # they are consistent. See tf-var-compliance-checker.sh for details
      - job: TFVariableComplianceChecker
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        steps:
          - checkout: self
            clean: true

          # Install terraform-docs from GitHub releases
          - bash: |

              # Check if terraform-docs is already installed
              if ! command -v terraform-docs &> /dev/null; then
                echo "terraform-docs not found. Installing..."
                # Detect architecture
                ARCH=$(uname -m)
                case $ARCH in
                  x86_64|amd64)
                    TERRAFORM_DOCS_ARCH="amd64"
                    ;;
                  aarch64|arm64)
                    TERRAFORM_DOCS_ARCH="arm64"
                    ;;
                  *)
                    echo "Unsupported architecture: $ARCH"
                    exit 1
                    ;;
                esac

                # Install terraform-docs
                echo "Installing terraform-docs for $TERRAFORM_DOCS_ARCH architecture..."
                curl -Lo ./terraform-docs.tar.gz "https://github.com/terraform-docs/terraform-docs/releases/download/v0.16.0/terraform-docs-v0.16.0-$(uname)-$TERRAFORM_DOCS_ARCH.tar.gz"
                tar -xzf terraform-docs.tar.gz
                chmod +x terraform-docs
                sudo mv terraform-docs /usr/local/bin/
              else
                echo "terraform-docs is already installed"
                terraform-docs --version
              fi

            displayName: Install terraform-docs
            condition: and( not(failed()), not(canceled()) )

          # Parse tf-vars-compliance-check.py output
          - bash: |
              variable_check_results=$(python3 ./scripts/tf-vars-compliance-check.py)
              echo "Variable Check Results: $variable_check_results"

              # Check if we got valid JSON results
              if echo "$variable_check_results" | jq empty 2>/dev/null; then
                # Parse each warning and output as an Azure Pipeline error
                echo "$variable_check_results" | jq -c '.[]' | while read -r result; do
                    variable=$(echo "$result" | jq -r '.variable')
                    # Join multiple folders with newlines and proper indentation
                    folders=$(echo "$result" | jq -r '.folders | join("\n  - ")')
                    # Format differences with newlines and indentation
                    differences=$(echo "$result" | jq -r '.differences | map("\n  - " + .) | join("")')
                    echo "##vso[task.logissue type=error]Variable '$variable' has inconsistencies in folders:\n  - $folders\n\nDifferences:$differences"
                done
              fi

            displayName: "Tf Variable Compliance Check"
            condition: and( not(failed()), not(canceled()) )

      # Job to run Terraform install, init, provider version checks, plan,
      # and tests for the changed TF folders
      - job: TerraformClusterTest
        dependsOn:
          - MatrixBuildFolderCheck
        pool:
          name: ai-on-edge-managed-pool
          vmImage: ubuntu-latest
        condition: eq(dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changesInInstall'], 'true')
        strategy:
          matrix:
            # Dynamically create a job for each folder in the changedTFFolders variable
            # there is some magic here as the changedTFFolders variable is a JSON object
            # Azure Pipeline strategy matrix can automatically parse JSON objects into
            # an enumerated matrix of values. This will create a $folderName variable
            # that can be used in the job steps to reference the current TF folder name
            # being processed.
            $[ dependencies.MatrixBuildFolderCheck.outputs['matrixBuildFolderCheckTask.changedTFFolders'] ]
          maxParallel: 1

        steps:
          - checkout: self
            clean: true

          # Initialize Terraform
          - task: TerraformInstaller@1
            displayName: "Install Terraform"
            inputs:
              terraformVersion: latest

          # Run terraform init with the established service connection in AzDO Pipeline config
          - task: TerraformCLI@1
            displayName: "Terraform Init - CI"
            inputs:
              command: init
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/ci/terraform/
              runAzLogin: true
              environmentServiceName: "azdo-ai-for-edge-iac-for-edge"
              backendType: azurerm
              # Service connection to authorize backend access. Supports Subscription & Management Group Scope
              backendServiceArm: "azdo-ai-for-edge-iac-for-edge"
              backendAzureRmResourceGroupName: "IaC_For_Edge"
              # azure location shortname of the backend resource group and storage account
              backendAzureRmResourceGroupLocation: "eastus"
              backendAzureRmStorageAccountName: "iacforedgetf"
              # azure blob container to store the state file
              backendAzureRmContainerName: "iacforedgetf"
              # azure blob file name
              backendAzureRmKey: infrax.tfstate

          # Run terraform init with the established service connection in AzDO Pipeline config
          - task: TerraformCLI@1
            displayName: "Terraform Init - Tests"
            inputs:
              command: init
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              runAzLogin: true
              environmentServiceName: "azdo-ai-for-edge-iac-for-edge"
              backendType: azurerm
              # Service connection to authorize backend access. Supports Subscription & Management Group Scope
              backendServiceArm: "azdo-ai-for-edge-iac-for-edge"
              backendAzureRmResourceGroupName: "IaC_For_Edge"
              # azure location shortname of the backend resource group and storage account
              backendAzureRmResourceGroupLocation: "eastus"
              backendAzureRmStorageAccountName: "iacforedgetf"
              # azure blob container to store the state file
              backendAzureRmContainerName: "iacforedgetf"
              # azure blob file name
              backendAzureRmKey: infrax.tfstate

          # Check TF Provider versions for this folder and output build warnings if issues are detected
          - task: AzureCLI@2
            displayName: "Terraform Provider version check"
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/ci/terraform/
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |

                # Get the provider details from the Terraform configuration
                # This will output a list of providers and their versions in the format:
                # folder,provider,version, latest_version, e.g.
                # ./src/030-iot-ops-cloud-reqs/terraform,azapi,2.1.0,2.2.0
                # ./src/040-iot-ops/terraform,azapi,2.1.0,2.2.0
                # Example version_check_results string
                version_check_results=$(./scripts/tf-provider-version-check.sh -f ./)

                # Parse the JSON and loop through each object
                echo "$version_check_results" | jq -c '.[]' | while read -r result; do
                  folder=$(echo "$result" | jq -r '.folder')
                  provider=$(echo "$result" | jq -r '.provider')
                  current_version=$(echo "$result" | jq -r '.current_version')
                  latest_version=$(echo "$result" | jq -r '.latest_version')

                  # Check if the current version does not match the latest version
                  if [[ "$current_version" != "$latest_version" ]]; then
                      echo "##vso[task.logissue type=error]Version mismatch in $folder for provider $provider: current version is $current_version, but latest version is $latest_version."
                  fi
                done
            condition: and( not(failed()), not(canceled()) )

          # Run Terraform Validate to ensure the TF files are valid
          # break if validation fails
          - task: TerraformCLI@1
            displayName: "Terraform Validate"
            inputs:
              command: validate
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/ci/terraform/
            continueOnError: false

          # Parse variables.tf and construct the specific commandOptions required for the plan call
          - bash: |

              # get the tf variable file from the current matrix directory
              variables_file=$(System.DefaultWorkingDirectory)/src/$(folderName)/ci/terraform/variables.tf
              # start the base command option for the coming TF Plan
              command_options="-out=$(Build.ArtifactStagingDirectory)/$(Build.Buildnumber).$(folderName).tfplan -detailed-exitcode"
              # if the variables file exists, parse it and for each variable,
              # see if we've set an AzDO pipeline variable to a matched name,
              # and then add that combination to that to the command options.
              # Special case is for Custom Locations OID because that is a
              # secrete value, and AzDO will not let you dynamically access
              # the variable name, you must know it, to map it.
              # e.g. https://stackoverflow.com/questions/75536144/azure-pipelines-error-tf401444-please-sign-in-at-least-once-as
              if [ -f "$variables_file" ]; then
                required_vars=$(grep -E 'variable "[^"]+" \{' $variables_file | awk -F'"' '{print $2}')
                for env_var in $(printenv | awk -F= '{print $1}'; echo "TF_VAR_CUSTOM_LOCATIONS_OID"); do
                  var_name=${env_var#TF_VAR_}
                  if echo "$required_vars" | grep -q "^${var_name,,}$"; then
                    command_options="$command_options -var ${var_name,,}=${!env_var}"
                  fi
                done
                # secretes will be automatically masked by AzDO
                echo $command_options
              fi
              echo "##vso[task.setvariable variable=commandOptions;isOutput=true]$command_options"
            displayName: "Parse variables.tf and build commandOptions"
            name: parseVariables
            condition: and( not(failed()), not(canceled()) )

          # Run Terraform Plan for reporting
          - task: TerraformCLI@1
            displayName: "Terraform Plan"
            inputs:
              command: plan
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/ci/terraform/
              runAzLogin: true
              environmentServiceName: "azdo-ai-for-edge-iac-for-edge"
              # this publish will drive the Terraform Plan UI on each build
              publishPlanResults: "$(folderName) - Terraform Test Plan"
              commandOptions: $(parseVariables.commandOptions)
            condition: and( not(failed()), not(canceled()) )

          # Publish the Terraform Plan as an official build artifact for downloading
          - task: PublishBuildArtifacts@1
            displayName: "Publish Artifact: $(folderName) Terraform Plan"
            inputs:
              PathtoPublish: "$(Build.ArtifactStagingDirectory)/$(Build.Buildnumber).$(folderName).tfplan"
              ArtifactName: "TerraformPlan"

          # Terraform Test Command
          # This is not yet implemented, tracking here: https://github.com/jason-johnson/azure-pipelines-tasks-terraform/issues/398
          # Once this command is available, the AZ CLI below can be removed
          # - task: TerraformCLI@2
          #   displayName: 'terraform test'
          #   inputs:
          #     command: test
          #     workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/tests"
          #     commandOptions: "-filter=cluster.tftest.hcl"

          # Run Terraform Test command using the Azure CLI
          # NOTE: For this task to work it needs the subscription id set for the AzureAD provider
          # beyond the integrated MSI.
          - task: AzureCLI@2
            name: TerraformTest
            displayName: Azure CLI for Terraform CLI test run
            inputs:
              azureSubscription: "azdo-ai-for-edge-iac-for-edge"
              workingDirectory: $(System.DefaultWorkingDirectory)/src/$(folderName)/terraform/
              scriptType: bash
              scriptLocation: inlineScript
              inlineScript: |

                # Export the ARM subscription ID so Terraform tests can be executed
                export ARM_SUBSCRIPTION_ID=$(SUBSCRIPTION_ID)

                # Define the log file from the test output to read up
                log_file="$(System.DefaultWorkingDirectory)/$(folderName)_test_output.log"
                echo "Writing Terraform test output to $log_file"

                # run TF tests and push results to log file
                terraform test -json -var custom_locations_oid=$(TF_VAR_CUSTOM_LOCATIONS_OID) > $log_file
                unset ARM_SUBSCRIPTION_ID

                # Read each line of the log file and extract the @timestamp and @message fields
                # this will be output to console in the build system
                while IFS= read -r line; do
                  echo "$line" | jq -r '"\(.["@timestamp"]) .... \(.["@message"])"'
                done < "$log_file"

                # Get the last line of the output and parse it for the word "Success"
                last_line=$(tail -n 1 "$log_file")
                if echo "$last_line" | jq -e '.["@message"]' | grep -q "Success"; then
                  echo "Terraform tests passed."
                else
                  echo "Terraform tests failed."
                  exit 1
                fi
            condition: and( not(failed()), not(canceled()) )

          # Need to use an external tool to convert the Terraform test output to JUnit XML
          # This tool is available as an NPM package and can be installed and run in the pipeline
          # But it requires Node.js to be installed on the agent
          - task: UseNode@1
            inputs:
              version: "22.x"
            condition: always()

          # Install the tftest-to-junitxml NPM package
          # https://github.com/Liam-Johnston/tftest-to-junitxml
          - script: |
              npm install -g tftest-to-junitxml
            displayName: "Install tftest-to-junitxml"
            condition: always()

          # Call the tool to convert the Terraform test output to JUnit XML
          - script: |
              npx tftest-to-junitxml "$(System.DefaultWorkingDirectory)/$(folderName)_test_output.log"
            displayName: "Convert Terraform test output to JUnit XML"
            condition: always()

          # Publish the Terraform test results as a build artifact to drive
          # the Test Results UI on each build. These will be merged with the
          # Pester Tests and other test results in the pipeline, but are
          # discretely visible in the Test Results UI.
          - task: PublishTestResults@2
            displayName: Publish Test Results
            inputs:
              testResultsFiles: "TEST-terraform.xml"
              testRunTitle: "Test Results for $(folderName)"
              buildPlatform: "Linux"
              testRunner: "JUnit"
              failTaskOnFailedTests: true
            condition: always()
